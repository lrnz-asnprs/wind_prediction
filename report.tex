\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

% Update this information to reflect yourself
\title{Assignment 1: Data Pipelines}
\author{Laurenz Aisenpreis}
\date{2021-09-29}

\begin{document}
\maketitle

\section{Introduction}

The following report discusses my implementation of a data pipeline to predict wind power production in Orkney. The data pipeline combines data from two different data sources, namely wind generation data and weather forecast data including the wind speed and direction.  
The application has been implemented by using the libraries \emph{sklearn} and \emph{pandas}.

\section{Pre-Pipeline Data Processing}

Some crucial preprocessing steps were performed before parsing the data into the pipeline. Most importantly, the timestamps of the two data sources had to be aligned. I took the decision, to query the wind generation data grouped by a time period of three hours and summed up the total wind power generated within this time frame.
Following this, the two data sources were merged and the columns \emph{Lead hours} and \emph{Source time} were dropped because they were not taken into consideration for the following data analysis. The main attributes for my analysis were therefore \emph{Speed} and \emph{Direction} to predict the \emph{Total} of generated wind. How those attributes were prepared is described in the next section.

\section{Data Pipeline}

In order to prepare the data set and conduct the prediction, both \emph{ColumnTransformer} and \emph{Pipeline} were used. Since the features of my model were a mix of numerical and categorical data, I decided to treat them differently. Within a \emph{ColumnTransformer}, first the missing values of the numerical feature \emph{Speed} were replaced by the median and scaled using the \emph{StandardScaler}. Secondly, the categorical feature \emph{Direction} was one hot encoded, to be able to interpret this attribute as a numerical value. \newline
Finally, my pipeline incorporated the transformations of the \emph{ColumnTransformer} and further transformed this data into polynomial combinations using \emph{PolynomialFeatures} to account for the non-linear nature of the input data. As a last step, I included the predictor of my choice, and analyzed how the linear regression and k-nearest neighbor regressor models perform on the data set.

\section{Prediction Models}

The linear regression model achieved a score of \emph{0.73}. Especially the inclusion of polynomial transformation led to an relative increase of \emph{12.3} percent of accuracy compared to a previous score of \emph{0.65}. This can be explained by a better fit of the polynomially transformed model due to the non-linear relationship of the underlying data. The picture below depicts the regression line of the fitted model on test data.




\section{Evaluation}

We conducted an experiment to evaluate the different implementations. To that end, we generated different datasets with different lengths, all containing only positive integers. As a result, the algorithms would not return any triples that sum to zero, allowing for a robust and comparable

\section{Potential Improvements}

We conducted an experiment to evaluate the different implementations. To that end, we generated different datasets with different lengths, all containing only positive integers. As a result, the algorithms would not return any triples that sum to zero, allowing for a robust and comparable


\begin{table}[h]
  \begin{center}
  \caption{This table depicts the average runtimes and standard deviations for different input sizes \emph{n} of the cubic algorithm in seconds}
  \label{tbl:resultscubic}
  %\input{threesum_cubic_tabular.tex}
  \end{center}
\end{table}

\clearpage

\section{Conclusion}
The results of the experiment support the intuition that the cubic algorithm takes much longer to compute than the hash map implementation. In fact, the running times differed for several orders of magnitude with increased input sizes.

\end{document}